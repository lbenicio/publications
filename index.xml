<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Home on Leonardo Benicio</title><link>https://lbenicio.dev/</link><description>Recent content in Home on Leonardo Benicio</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 26 Oct 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://lbenicio.dev/index.xml" rel="self" type="application/rss+xml"/><item><title>Inside Vector Databases: Building Retrieval-Augmented Systems that Scale</title><link>https://lbenicio.dev/blog/inside-vector-databases-building-retrieval-augmented-systems-that-scale/</link><pubDate>Sun, 26 Oct 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/inside-vector-databases-building-retrieval-augmented-systems-that-scale/</guid><description>&lt;p&gt;Vector search used to be a research curiosity. Today it sits in the critical path of customer support bots, developer copilots, fraud monitors, and every product marketing team experimenting with &amp;ldquo;retrieval-augmented&amp;rdquo; workflows. The excitement is deserved, but so is the sober engineering required to keep these systems accurate and available. Building a production vector database is more than storing tensors and calling cosine similarity. It demands a full stack of ingestion, indexing, storage management, failure handling, evaluation, and a constant feedback loop with the language models that consume those results.&lt;/p&gt;</description></item><item><title>Learned Indexes: When Models Replace B‑Trees</title><link>https://lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/</link><pubDate>Sat, 04 Oct 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/learned-indexes-when-models-replace-btrees/</guid><description>&lt;p&gt;If you’ve spent a career trusting B‑trees and hash tables, the idea of using a machine‑learned model as an index can feel like swapping a torque wrench for a Ouija board. But learned indexes aren’t a gimmick. They exploit a simple observation: real data isn’t uniformly random. It has shape—monotonic keys, skewed distributions, natural clusters—and a model can learn that shape to predict where a key lives in a sorted array. The payoff is smaller indexes, fewer cache misses, and—sometimes—dramatically faster lookups.&lt;/p&gt;</description></item><item><title>The 100‑Microsecond Rule: Why Tail Latency Eats Your Throughput (and How to Fight Back)</title><link>https://lbenicio.dev/blog/the-100microsecond-rule-why-tail-latency-eats-your-throughput-and-how-to-fight-back/</link><pubDate>Sat, 04 Oct 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-100microsecond-rule-why-tail-latency-eats-your-throughput-and-how-to-fight-back/</guid><description>&lt;p&gt;If you stare at a performance dashboard long enough, you’ll eventually see a ghost—an outlier that refuses to go away. It’s the P99 spike that surfaces at the worst time; the one you “fix” three times and then rediscover during a product launch or a perfectly normal Tuesday.&lt;/p&gt;
&lt;p&gt;Here’s the hard truth: tail latency doesn’t just ruin your service levels; it compounds into lost throughput and broken guarantees. In systems with fan‑out, retries, and microservices, the slowest 1% isn’t “rare”—it’s the norm you ship to users most of the time. This is the 100‑microsecond rule in practice: small latencies multiply brutally at scale, and the invisible cost often starts below a single millisecond.&lt;/p&gt;</description></item><item><title>The Quiet Calculus of Probabilistic Commutativity</title><link>https://lbenicio.dev/blog/the-quiet-calculus-of-probabilistic-commutativity/</link><pubDate>Sat, 27 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-quiet-calculus-of-probabilistic-commutativity/</guid><description>&lt;h2 id="abstract"&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Eventual consistency dominates many internet-scale systems, but reasoning about concurrency under minimal coordination remains ad hoc. This post introduces &amp;ldquo;probabilistic commutativity&amp;rdquo; — a lightweight calculus for reasoning about whether concurrent operations, under reasonable stochastic assumptions about ordering and visibility delays, are likely to commute in practice. Probabilistic commutativity offers an intermediate lens between strict algebraic commutativity and empirical test-driven guarantees, enabling low-overhead coordination strategies and probabilistic correctness arguments for producing practically consistent distributed services.&lt;/p&gt;</description></item><item><title>The Hidden Backbone of Parallelism: How Prefix Sums Power Distributed Computation</title><link>https://lbenicio.dev/blog/the-hidden-backbone-of-parallelism-how-prefix-sums-power-distributed-computation/</link><pubDate>Sun, 21 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/the-hidden-backbone-of-parallelism-how-prefix-sums-power-distributed-computation/</guid><description>&lt;h2 id="introduction"&gt;Introduction&lt;/h2&gt;
&lt;p&gt;When most people think about parallel computing, they imagine splitting a massive task into smaller chunks and running them simultaneously. That’s the Hollywood version: thousands of processors blazing through data, crunching numbers in parallel, and finishing jobs in seconds.&lt;/p&gt;
&lt;p&gt;The reality is both more fascinating and more subtle. Beneath the surface of supercomputers and distributed systems lies a set of seemingly modest mathematical operations—building blocks that make large-scale parallelism possible. Among these, one stands out for its simplicity and profound impact: the &lt;strong&gt;parallel prefix sum&lt;/strong&gt;, also called a &lt;em&gt;scan&lt;/em&gt;.&lt;/p&gt;</description></item><item><title>GPUDirect Storage in 2025: Optimizing the End-to-End Data Path</title><link>https://lbenicio.dev/blog/gpudirect-storage-in-2025-optimizing-the-end-to-end-data-path/</link><pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/gpudirect-storage-in-2025-optimizing-the-end-to-end-data-path/</guid><description>&lt;p&gt;High-performance analytics and training pipelines increasingly hinge on how &lt;em&gt;fast&lt;/em&gt; and &lt;em&gt;efficiently&lt;/em&gt; data reaches GPU memory. Compute has outpaced I/O: a single multi-GPU node can sustain tens of TFLOPs while starved by a few misconfigured storage or copy stages. GPUDirect Storage (GDS) extends the GPUDirect family (peer-to-peer, RDMA) to allow DMA engines (NVMe, NIC) to move bytes directly between storage and GPU memory—bypassing redundant copies through host DRAM and reducing CPU intervention.&lt;/p&gt;</description></item><item><title>MPI vs. OpenMP in 2025: Where Each Wins</title><link>https://lbenicio.dev/blog/mpi-vs.-openmp-in-2025-where-each-wins/</link><pubDate>Fri, 04 Jul 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/mpi-vs.-openmp-in-2025-where-each-wins/</guid><description>&lt;p&gt;Modern clusters have fat nodes (many cores, large memory) and fast interconnects. That’s why hybrid patterns—MPI between nodes and OpenMP within a node—are common.&lt;/p&gt;
&lt;h3 id="when-mpi-wins"&gt;When MPI wins&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Distributed memory by necessity: datasets exceed a node’s RAM.&lt;/li&gt;
&lt;li&gt;Clear data ownership and minimal sharing.&lt;/li&gt;
&lt;li&gt;Coarse-grained decomposition with small surface/volume ratio.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="when-openmp-wins"&gt;When OpenMP wins&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Shared-memory parallel loops and tasks with modest synchronization.&lt;/li&gt;
&lt;li&gt;NUMA-aware data placement still local to a node.&lt;/li&gt;
&lt;li&gt;Rapid prototyping and incremental parallelization of CPU-bound kernels.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="hybrid-design-tips"&gt;Hybrid design tips&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bind MPI ranks to NUMA domains; spawn OpenMP threads within each. Avoid oversubscription.&lt;/li&gt;
&lt;li&gt;Use non-blocking collectives (MPI_Iallreduce) to hide latency.&lt;/li&gt;
&lt;li&gt;Pin memory and leverage huge pages where available.&lt;/li&gt;
&lt;li&gt;Profile: check MPI time vs. compute vs. OpenMP overhead; fix the biggest slice first.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="example-openmp-reduction-vs-mpi-allreduce"&gt;Example: OpenMP reduction vs. MPI allreduce&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre tabindex="0" style="color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"&gt;&lt;code class="language-c" data-lang="c"&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#8b949e;font-style:italic"&gt;// OpenMP reduction inside a rank
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#8b949e;font-style:italic"&gt;&lt;/span&gt;&lt;span style="color:#8b949e;font-weight:bold;font-style:italic"&gt;#pragma omp parallel for reduction(+:sum)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#8b949e;font-weight:bold;font-style:italic"&gt;&lt;/span&gt;&lt;span style="color:#ff7b72"&gt;for&lt;/span&gt; (&lt;span style="color:#ff7b72"&gt;int&lt;/span&gt; i &lt;span style="color:#ff7b72;font-weight:bold"&gt;=&lt;/span&gt; &lt;span style="color:#a5d6ff"&gt;0&lt;/span&gt;; i &lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;lt;&lt;/span&gt; n; &lt;span style="color:#ff7b72;font-weight:bold"&gt;++&lt;/span&gt;i) sum &lt;span style="color:#ff7b72;font-weight:bold"&gt;+=&lt;/span&gt; a[i] &lt;span style="color:#ff7b72;font-weight:bold"&gt;*&lt;/span&gt; b[i];
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#8b949e;font-style:italic"&gt;// Then global reduce across ranks
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style="display:flex;"&gt;&lt;span&gt;&lt;span style="color:#8b949e;font-style:italic"&gt;&lt;/span&gt;&lt;span style="color:#d2a8ff;font-weight:bold"&gt;MPI_Allreduce&lt;/span&gt;(&lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;sum, &lt;span style="color:#ff7b72;font-weight:bold"&gt;&amp;amp;&lt;/span&gt;global, &lt;span style="color:#a5d6ff"&gt;1&lt;/span&gt;, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Rule of thumb: start simple (single model) and add hybrid complexity only when measurements show you must.&lt;/p&gt;</description></item><item><title>From MapReduce to Spark: The Arc of Data-Parallel Systems</title><link>https://lbenicio.dev/blog/from-mapreduce-to-spark-the-arc-of-data-parallel-systems/</link><pubDate>Mon, 19 May 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/from-mapreduce-to-spark-the-arc-of-data-parallel-systems/</guid><description>&lt;p&gt;MapReduce popularized large-scale batch processing with a simple model (map, shuffle, reduce) and immutable intermediate state on HDFS. It optimized for throughput and fault tolerance via re-execution.&lt;/p&gt;
&lt;p&gt;Spark expanded the model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RDDs: immutable, partitioned datasets with lineage, enabling recomputation on failure.&lt;/li&gt;
&lt;li&gt;DAG scheduler: plans multi-stage jobs, pipelining narrow transformations and materializing wide ones.&lt;/li&gt;
&lt;li&gt;In-memory caching: keeps hot datasets in RAM to accelerate iterative workloads.&lt;/li&gt;
&lt;li&gt;Higher-level APIs: DataFrames/Datasets and SQL, plus MLlib and Structured Streaming.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="checkpointing-and-lineage"&gt;Checkpointing and lineage&lt;/h3&gt;
&lt;p&gt;RDD lineage can grow large; Spark checkpoints to cut recomputation cost. For streaming, write-ahead logs plus checkpoints enable recovery.&lt;/p&gt;</description></item><item><title>Auditing the Algorithm: Building a Responsible AI Pipeline That Scales</title><link>https://lbenicio.dev/blog/auditing-the-algorithm-building-a-responsible-ai-pipeline-that-scales/</link><pubDate>Sat, 05 Apr 2025 13:25:00 +0000</pubDate><guid>https://lbenicio.dev/blog/auditing-the-algorithm-building-a-responsible-ai-pipeline-that-scales/</guid><description>&lt;p&gt;When regulators asked for evidence that our AI systems behave responsibly, we realized spreadsheets and ad hoc reviews wouldn&amp;rsquo;t suffice. Our models influenced credit decisions, hiring suggestions, and medical triage. Stakeholders demanded more than accuracy—they wanted fairness, explainability, and accountability. We responded by building a responsible AI audit pipeline woven into development, deployment, and operations. This article shares how we did it: the processes, tooling, and cultural shifts that turned compliance into continuous curiosity.&lt;/p&gt;</description></item><item><title>Scheduling: Trading Latency for Throughput (and Back Again)</title><link>https://lbenicio.dev/blog/scheduling-trading-latency-for-throughput-and-back-again/</link><pubDate>Wed, 12 Feb 2025 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/scheduling-trading-latency-for-throughput-and-back-again/</guid><description>&lt;p&gt;Schedulers encode policy: who runs next, on which core, and for how long. Those choices shuffle latency and throughput. Let’s make the trade‑offs explicit.&lt;/p&gt;
&lt;h2 id="fifo-vs-priority-vs-fair"&gt;FIFO vs. priority vs. fair&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;FIFO: simple, minimal overhead; tail prone under bursty arrivals.&lt;/li&gt;
&lt;li&gt;Priority: protects critical work; risks starvation without aging.&lt;/li&gt;
&lt;li&gt;Fair (CFQ‑like): shares CPU evenly; may underutilize when work is imbalanced.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="work-stealing"&gt;Work stealing&lt;/h2&gt;
&lt;p&gt;Great for irregular parallel workloads. Each worker has a deque; thieves steal from the tail. Pros: high utilization; Cons: cache locality losses and noisy tails under high contention.&lt;/p&gt;</description></item><item><title>Exactly-Once in Streaming: What It Means and How Systems Achieve It</title><link>https://lbenicio.dev/blog/exactly-once-in-streaming-what-it-means-and-how-systems-achieve-it/</link><pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/exactly-once-in-streaming-what-it-means-and-how-systems-achieve-it/</guid><description>&lt;p&gt;&amp;ldquo;Exactly-once&amp;rdquo; doesn’t mean an event packet traverses the network a single time. It means that in the &lt;em&gt;observable&lt;/em&gt; outputs (state, emitted records, external side effects) each logical input is reflected &lt;strong&gt;at most once&lt;/strong&gt; and &lt;strong&gt;at least once&lt;/strong&gt;—so exactly once—despite retries, replays, failovers, or speculative execution. It is an &lt;em&gt;end-to-end&lt;/em&gt; property requiring cooperation across producer, broker, processing engine, and sinks.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="1-taxonomy-of-delivery--processing-guarantees"&gt;1. Taxonomy of Delivery &amp;amp; Processing Guarantees&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;Term&lt;/th&gt;
 &lt;th&gt;Network Delivery&lt;/th&gt;
 &lt;th&gt;Processing Attempts&lt;/th&gt;
 &lt;th&gt;External Effect Guarantee&lt;/th&gt;
 &lt;th&gt;Typical Mechanisms&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;At most once&lt;/td&gt;
 &lt;td&gt;Messages may be lost&lt;/td&gt;
 &lt;td&gt;≤1&lt;/td&gt;
 &lt;td&gt;Missing effects possible&lt;/td&gt;
 &lt;td&gt;Fire-and-forget, no ack replay&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;At least once&lt;/td&gt;
 &lt;td&gt;Redelivery possible&lt;/td&gt;
 &lt;td&gt;≥1&lt;/td&gt;
 &lt;td&gt;Duplicated effects if not idempotent&lt;/td&gt;
 &lt;td&gt;Offsets/checkpoints + replay&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Exactly once (processing)&lt;/td&gt;
 &lt;td&gt;Redelivery possible&lt;/td&gt;
 &lt;td&gt;≥1&lt;/td&gt;
 &lt;td&gt;Each logical record causes effect once&lt;/td&gt;
 &lt;td&gt;Idempotence + dedupe or transactions + snapshots&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Note the &lt;em&gt;processing&lt;/em&gt; dimension: we usually allow redelivery at the transport layer; the system masks duplicates before they become externally visible.&lt;/p&gt;</description></item><item><title>Tuning CUDA with the GPU Memory Hierarchy</title><link>https://lbenicio.dev/blog/tuning-cuda-with-the-gpu-memory-hierarchy/</link><pubDate>Wed, 27 Nov 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/tuning-cuda-with-the-gpu-memory-hierarchy/</guid><description>&lt;p&gt;CUDA performance hinges on moving data efficiently through a &lt;em&gt;hierarchy&lt;/em&gt; of memories that differ by latency, bandwidth, scope (visibility), and capacity. Raw FLOP throughput is rarely the first limiter—memory behavior, access ordering, and reuse patterns almost always dominate performance envelopes.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="1-the-hierarchy-at-a-glance"&gt;1. The Hierarchy at a Glance&lt;/h2&gt;
&lt;table&gt;
 &lt;thead&gt;
 &lt;tr&gt;
 &lt;th&gt;Level&lt;/th&gt;
 &lt;th&gt;Scope / Visibility&lt;/th&gt;
 &lt;th&gt;Approx Latency (cycles)*&lt;/th&gt;
 &lt;th&gt;Bandwidth&lt;/th&gt;
 &lt;th&gt;Capacity (per SM / device)&lt;/th&gt;
 &lt;th&gt;Notes&lt;/th&gt;
 &lt;/tr&gt;
 &lt;/thead&gt;
 &lt;tbody&gt;
 &lt;tr&gt;
 &lt;td&gt;Registers&lt;/td&gt;
 &lt;td&gt;Thread private&lt;/td&gt;
 &lt;td&gt;~1&lt;/td&gt;
 &lt;td&gt;Extreme&lt;/td&gt;
 &lt;td&gt;Tens of k per SM (allocated per thread)&lt;/td&gt;
 &lt;td&gt;Allocation affects occupancy; spilling -&amp;gt; local memory&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Shared Memory (SMEM)&lt;/td&gt;
 &lt;td&gt;Block (CTA)&lt;/td&gt;
 &lt;td&gt;~20–35&lt;/td&gt;
 &lt;td&gt;Very high&lt;/td&gt;
 &lt;td&gt;48–228 KB configurable (arch dependent)&lt;/td&gt;
 &lt;td&gt;Banked; subject to conflicts; optional split w/ L1&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;L1 / Texture Cache&lt;/td&gt;
 &lt;td&gt;SM&lt;/td&gt;
 &lt;td&gt;~30–60&lt;/td&gt;
 &lt;td&gt;High&lt;/td&gt;
 &lt;td&gt;~128–256 KB (unified)&lt;/td&gt;
 &lt;td&gt;Serves global loads; spatial locality &amp;amp; coalescing still matter&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;L2 Cache&lt;/td&gt;
 &lt;td&gt;Device-wide&lt;/td&gt;
 &lt;td&gt;~200–300&lt;/td&gt;
 &lt;td&gt;High&lt;/td&gt;
 &lt;td&gt;Multi-MB&lt;/td&gt;
 &lt;td&gt;Coherent across SMs; crucial for global data reuse&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Global DRAM&lt;/td&gt;
 &lt;td&gt;Device-wide&lt;/td&gt;
 &lt;td&gt;~400–800&lt;/td&gt;
 &lt;td&gt;High (GB/s)&lt;/td&gt;
 &lt;td&gt;Many GB&lt;/td&gt;
 &lt;td&gt;Long latency—hide with parallelism &amp;amp; coalescing&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Constant Cache&lt;/td&gt;
 &lt;td&gt;Device-wide (read-only)&lt;/td&gt;
 &lt;td&gt;~ L1 hit if cached&lt;/td&gt;
 &lt;td&gt;High (broadcast)&lt;/td&gt;
 &lt;td&gt;64 KB&lt;/td&gt;
 &lt;td&gt;Broadcast to warp if all threads read same address&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Texture / Read-Only Cache&lt;/td&gt;
 &lt;td&gt;Device-wide (cached)&lt;/td&gt;
 &lt;td&gt;Similar to L1&lt;/td&gt;
 &lt;td&gt;High&lt;/td&gt;
 &lt;td&gt;N/A&lt;/td&gt;
 &lt;td&gt;Provides specialized spatial filtering &amp;amp; relaxed coalescing&lt;/td&gt;
 &lt;/tr&gt;
 &lt;tr&gt;
 &lt;td&gt;Local Memory&lt;/td&gt;
 &lt;td&gt;Thread (spill/backing)&lt;/td&gt;
 &lt;td&gt;DRAM latency&lt;/td&gt;
 &lt;td&gt;DRAM&lt;/td&gt;
 &lt;td&gt;Per-thread virtual&lt;/td&gt;
 &lt;td&gt;“Local” is misnomer if spilled—same as global latency&lt;/td&gt;
 &lt;/tr&gt;
 &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;em&gt;Indicative ranges; varies by architecture generation (e.g., Turing, Ampere, Hopper). Absolute numbers less important than ratio gaps.&lt;/em&gt;&lt;/p&gt;</description></item><item><title>Seeing in the Dark: Observability for Edge AI Fleets</title><link>https://lbenicio.dev/blog/seeing-in-the-dark-observability-for-edge-ai-fleets/</link><pubDate>Fri, 16 Aug 2024 10:55:00 +0000</pubDate><guid>https://lbenicio.dev/blog/seeing-in-the-dark-observability-for-edge-ai-fleets/</guid><description>&lt;p&gt;Our edge AI deployment began with a handful of pilot devices in retail stores. Within months, thousands of cameras, sensors, and point-of-sale terminals joined the fleet. They detected shelves running low, predicted queue lengths, and flagged suspicious transactions. But when a customer called asking why a device misclassified bananas as tennis balls, we realized our observability blurred at the edge. Logs vanished into the ether, metrics arrived sporadically, and models drifted silently. This article shares how we built observability robust enough for flaky networks, sensitive data, and autonomous updates.&lt;/p&gt;</description></item><item><title>Adaptive Feature Flag Frameworks for Hyper-Growth SaaS</title><link>https://lbenicio.dev/blog/adaptive-feature-flag-frameworks-for-hyper-growth-saas/</link><pubDate>Thu, 15 Aug 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/adaptive-feature-flag-frameworks-for-hyper-growth-saas/</guid><description>&lt;p&gt;Hyper-growth SaaS companies live in perpetual motion. Every day brings new customer segments, compliance regimes, infrastructure bottlenecks, and competitive threats. Feature flags evolved from simple booleans to mission-critical systems that guard deployments, power experiments, share risk across teams, and surface real-time signals about user delight. Building an adaptive feature flag framework is no longer optional; it is the backbone of progressive delivery and organizational learning. This article offers a deep dive into the patterns, guardrails, and operating models needed to keep feature flags safe, observable, and aligned with business outcomes when your product is shipping at warp speed.&lt;/p&gt;</description></item><item><title>Amdahl’s Law vs. Gustafson’s Law: What They Really Predict</title><link>https://lbenicio.dev/blog/amdahls-law-vs.-gustafsons-law-what-they-really-predict/</link><pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/amdahls-law-vs.-gustafsons-law-what-they-really-predict/</guid><description>&lt;p&gt;Amdahl’s Law and Gustafson’s Law are often presented as opposites, but they model different scenarios.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amdahl assumes a fixed problem size and asks: how much faster can I make this workload with P processors when a fraction s is inherently serial? The upper bound is:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\text{Speedup}_A(P) = \frac{1}{s + \frac{1-s}{P}}.$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Gustafson assumes a fixed execution time and asks: given P processors, how much bigger can the problem become if parallel parts scale? The scaled speedup is:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\text{Speedup}_G(P) = s + (1-s),P.$$&lt;/p&gt;</description></item><item><title>Countdown to Quantum: Migrating an Enterprise to Post-Quantum Cryptography</title><link>https://lbenicio.dev/blog/countdown-to-quantum-migrating-an-enterprise-to-post-quantum-cryptography/</link><pubDate>Mon, 29 Jan 2024 16:40:00 +0000</pubDate><guid>https://lbenicio.dev/blog/countdown-to-quantum-migrating-an-enterprise-to-post-quantum-cryptography/</guid><description>&lt;p&gt;When the first credible quantum threat report reached our CISO&amp;rsquo;s desk, the reaction was cautious curiosity. Quantum computers capable of breaking RSA-2048 remain years away, but data harvested today could be decrypted decades later. We handle sensitive customer contracts, healthcare records, and government workloads. Waiting for the quantum threat to manifest felt irresponsible. So we embarked on a multi-year migration to post-quantum cryptography (PQC). This post documents the journey—strategy, tooling, experiments, and the human moments that kept the program afloat.&lt;/p&gt;</description></item><item><title>Sealing the Supply Chain: Zero-Trust Build Pipelines That Scale</title><link>https://lbenicio.dev/blog/sealing-the-supply-chain-zero-trust-build-pipelines-that-scale/</link><pubDate>Sun, 08 Oct 2023 21:10:00 +0000</pubDate><guid>https://lbenicio.dev/blog/sealing-the-supply-chain-zero-trust-build-pipelines-that-scale/</guid><description>&lt;p&gt;The day we discovered a compromised dependency in our build pipeline felt like waking up to find the locks on our headquarters replaced. The package arrived through a trusted mirror, signed with a familiar key, yet embedded a subtle time bomb. We escaped without production impact thanks to layered defenses, but the scare pushed us to redesign our build system around zero trust. This article chronicles that transformation—technical and human—from ad hoc pipelines to hermetic, verifiable, auditable delivery.&lt;/p&gt;</description></item><item><title>Reverse Indexing and Inverted Files: How Search Engines Fly</title><link>https://lbenicio.dev/blog/reverse-indexing-and-inverted-files-how-search-engines-fly/</link><pubDate>Wed, 19 Jul 2023 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/reverse-indexing-and-inverted-files-how-search-engines-fly/</guid><description>&lt;p&gt;Full‑text search is a masterclass in practical data structures. The inverted index—also called a reverse index—maps terms to the list of documents in which they occur. Everything else in a production search engine is optimization: reducing bytes, minimizing random I/O, and avoiding work you don’t have to do.&lt;/p&gt;
&lt;p&gt;In this deep dive we’ll build a complete mental model of inverted files and the techniques that make them fast:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parsing pipeline: tokenization, normalization, stemming/lemmatization, and multilingual realities&lt;/li&gt;
&lt;li&gt;Index structure: vocabulary, postings (doc IDs), term frequencies, and positions for phrase queries&lt;/li&gt;
&lt;li&gt;Compression: delta encoding, Variable‑Byte (VB), Simple‑8b, PForDelta, SIMD‑BP128, QMX, and how they trade space for CPU&lt;/li&gt;
&lt;li&gt;Skipping and acceleration: skip lists, block max indexes, WAND/BMW dynamic pruning&lt;/li&gt;
&lt;li&gt;Scoring: BM25, term and document statistics, field boosts, and normalization&lt;/li&gt;
&lt;li&gt;Updates and merges: segment architecture (Lucene‑style), in‑place deletes, and background compaction&lt;/li&gt;
&lt;li&gt;Caching and tiering: hot vs cold shards, result caching, and Bloom‑like structures&lt;/li&gt;
&lt;li&gt;Distributed search: sharding, replication, and query fan‑out under tail latency pressure&lt;/li&gt;
&lt;li&gt;Measuring and tuning: from recall/precision to p95 query time, heap usage, and GC pauses&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By the end you’ll be able to reason about why each knob exists and which ones matter for your workload.&lt;/p&gt;</description></item><item><title>Latency-Aware Edge Inference Platforms: Engineering Consistent AI Experiences</title><link>https://lbenicio.dev/blog/latency-aware-edge-inference-platforms-engineering-consistent-ai-experiences/</link><pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/latency-aware-edge-inference-platforms-engineering-consistent-ai-experiences/</guid><description>&lt;p&gt;Large language models, recommender systems, and computer vision pipelines increasingly run where users are: on kiosks, factory floors, AR headsets, connected vehicles, and retail shelves. But pushing models to the edge without a latency-aware strategy is a recipe for jittery UX, missed detections, and compliance headaches. This guide dissects what it takes to build an edge inference platform that meets strict latency budgets—even when networks wobble, models evolve weekly, and hardware varies wildly.&lt;/p&gt;</description></item><item><title>Keeping the Model Awake: Building a Self-Healing ML Inference Platform</title><link>https://lbenicio.dev/blog/keeping-the-model-awake-building-a-self-healing-ml-inference-platform/</link><pubDate>Tue, 14 Feb 2023 07:20:00 +0000</pubDate><guid>https://lbenicio.dev/blog/keeping-the-model-awake-building-a-self-healing-ml-inference-platform/</guid><description>&lt;p&gt;During a winter holiday freeze, our recommendation API refused to scale. GPUs idled waiting for models to load, autoscalers fought each other, and on-call engineers reheated leftovers at 3 a.m. while spike traffic slammed into origin. We promised leadership that this would never happen again. The solution wasn&amp;rsquo;t magic; it was a self-healing inference platform blending old-school reliability, modern ML tooling, and relentless experimentation.&lt;/p&gt;
&lt;p&gt;This post documents the rebuild. We&amp;rsquo;ll explore model packaging, warm-up rituals, adaptive scheduling, observability, chaos drills, and the social contract between ML researchers and production engineers. The goal: keep models awake, snappy, and trustworthy even when the world throws curveballs.&lt;/p&gt;</description></item><item><title>Timeouts, Retries, and Idempotency Keys: A Practical Guide</title><link>https://lbenicio.dev/blog/timeouts-retries-and-idempotency-keys-a-practical-guide/</link><pubDate>Thu, 08 Sep 2022 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/timeouts-retries-and-idempotency-keys-a-practical-guide/</guid><description>&lt;p&gt;Every distributed call needs three decisions: how long to wait, how to retry, and how to avoid duplicate effects. Done right, these three give you calm dashboards during partial failures; done wrong, they turn blips into incidents. This is a practical guide you can wire into client libraries and services without heroics.&lt;/p&gt;
&lt;h2 id="budget-your-time-not-perhop-timeouts"&gt;Budget your time, not per‑hop timeouts&lt;/h2&gt;
&lt;p&gt;Propagate a request deadline. Each hop consumes a slice and passes the remainder. Treat it as a budget: if it’s nearly spent, fail fast. Per‑hop fixed timeouts accumulate and blow your SLOs because each layer waits its entire local timeout.&lt;/p&gt;</description></item><item><title>Teaching GraphQL to Cache at the Edge</title><link>https://lbenicio.dev/blog/teaching-graphql-to-cache-at-the-edge/</link><pubDate>Sat, 03 Sep 2022 12:15:00 +0000</pubDate><guid>https://lbenicio.dev/blog/teaching-graphql-to-cache-at-the-edge/</guid><description>&lt;p&gt;GraphQL promises tailor-made responses, but tailor-made payloads resist caching. For years, we treated GraphQL responses as ephemeral: generated on demand, personalized, too unique to reuse. Then mobile latency complaints reached a boiling point. Edge locations sat underutilized while origin clusters sweated. We set out to teach GraphQL how to cache—respecting declarative queries, personalization boundaries, and real-time freshness. This is the story of building an edge caching layer that felt invisible to developers yet shaved hundreds of milliseconds off user interactions.&lt;/p&gt;</description></item><item><title>Designing CRDT-Powered Collaboration Platforms that Stay Consistent</title><link>https://lbenicio.dev/blog/designing-crdt-powered-collaboration-platforms-that-stay-consistent/</link><pubDate>Wed, 17 Aug 2022 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/designing-crdt-powered-collaboration-platforms-that-stay-consistent/</guid><description>&lt;p&gt;Real-time collaboration has shifted from a feature to a baseline expectation. Teams sketch ideas in shared whiteboards, edit code together, and annotate product specs while sitting on unreliable networks. Delivering that experience demands more than WebSockets and optimistic UI code. The hard part is consistency: making sure each participant converges on the same state despite offline edits, concurrent operations, and shaky connectivity. Conflict-free replicated data types (CRDTs) emerged as the workhorse that keeps these experiences coherent.&lt;/p&gt;</description></item><item><title>Instrumenting Without Spying: Privacy-Preserving Telemetry at Scale</title><link>https://lbenicio.dev/blog/instrumenting-without-spying-privacy-preserving-telemetry-at-scale/</link><pubDate>Thu, 27 May 2021 18:45:00 +0000</pubDate><guid>https://lbenicio.dev/blog/instrumenting-without-spying-privacy-preserving-telemetry-at-scale/</guid><description>&lt;p&gt;Three years ago, our observability dashboards flickered ominously: ingestion lag, missing metrics, red error bars. We had paused telemetry from millions of devices after discovering that our pipeline collected more than we were comfortable storing. The pause kept our promise to users—but left engineers blind. We needed a way to reinstate observability without betraying trust. The result was a radical redesign: a privacy-preserving telemetry system that treats data minimization as a feature, not a compliance chore.&lt;/p&gt;</description></item><item><title>Deterministic Monorepo CI Platforms: Engineering Consistency at Scale</title><link>https://lbenicio.dev/blog/deterministic-monorepo-ci-platforms-engineering-consistency-at-scale/</link><pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/deterministic-monorepo-ci-platforms-engineering-consistency-at-scale/</guid><description>&lt;p&gt;Monorepos promise unified visibility, atomic changes, and a single source of truth. They also create CI/CD nightmares if builds become flaky, nondeterministic, or painfully slow. Deterministic CI platforms tame that chaos by ensuring every pipeline run produces the same outputs given the same inputs—no matter which engineer kicks it off, which runner executes it, or when it happens. This article is a long-form walkthrough for platform teams charged with shipping deterministic CI in sprawling monorepos covering mobile apps, microservices, infrastructure code, and ML assets. Expect hard-earned lessons, architectural blueprints, and operational playbooks drawn from companies that run tens of thousands of jobs per day without losing their sanity.&lt;/p&gt;</description></item><item><title>Cache‑Friendly Data Layouts: AoS vs. SoA (and the Hybrid In‑Between)</title><link>https://lbenicio.dev/blog/cachefriendly-data-layouts-aos-vs.-soa-and-the-hybrid-inbetween/</link><pubDate>Thu, 18 Mar 2021 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/cachefriendly-data-layouts-aos-vs.-soa-and-the-hybrid-inbetween/</guid><description>&lt;p&gt;The fastest code you’ll ever write usually isn’t a brand‑new algorithm—it’s the same algorithm organized in memory so the CPU (or GPU) can consume it efficiently. That journey often starts with a deceptively small choice: Array‑of‑Structs (AoS) or Struct‑of‑Arrays (SoA). The layout you choose determines which cachelines move, which prefetchers trigger, how branch predictors behave, and whether SIMD units stay busy or starve.&lt;/p&gt;
&lt;p&gt;In this post we’ll build an intuitive mental model for cachelines, TLBs, prefetchers, and vector units; examine AoS and SoA under realistic access patterns; then derive a hybrid (AoSoA) that quietly powers many high‑performance systems—from physics engines to databases to ML dataloaders. We’ll also walk through migration strategies, benchmarking pitfalls, and checklists you can apply this week.&lt;/p&gt;</description></item><item><title>Raft Fast‑Commit and PreVote in Practice</title><link>https://lbenicio.dev/blog/raft-fastcommit-and-prevote-in-practice/</link><pubDate>Mon, 09 Nov 2020 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/raft-fastcommit-and-prevote-in-practice/</guid><description>&lt;p&gt;Raft’s original paper optimizes for understandability. Production clusters optimize for availability and mean‑time‑to‑recovery. Two common extensions—PreVote and fast‑commit—reduce needless disruptions and trim the time it takes to make progress. Let’s unpack them without hand‑waving.&lt;/p&gt;
&lt;h2 id="the-pain-disruptive-elections-and-long-commit-paths"&gt;The pain: disruptive elections and long commit paths&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Without PreVote, a partitioned follower may increment term and trigger elections on rejoin, ousting a healthy leader.&lt;/li&gt;
&lt;li&gt;Without fast‑commit, clients wait for the full round‑trip alignment even when a quorum already holds the entry durably.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="prevote"&gt;PreVote&lt;/h2&gt;
&lt;p&gt;PreVote adds a “dry‑run” phase: before incrementing term, a candidate asks peers if they’d vote. Peers reject if their logs are more up‑to‑date.&lt;/p&gt;</description></item><item><title>Safe Rollback Strategies for Distributed Databases</title><link>https://lbenicio.dev/blog/safe-rollback-strategies-for-distributed-databases/</link><pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/safe-rollback-strategies-for-distributed-databases/</guid><description>&lt;p&gt;Rollbacks are the fire escapes of distributed databases. We hope never to use them, but when outages, migrations, or bad deployments hit, a well-practiced rollback can save hours of business downtime and piles of customer tickets. Unfortunately, many organizations treat rollback planning as an afterthought—&amp;ldquo;we&amp;rsquo;ll just restore from backup&amp;rdquo;—only to discover data drift, cascading failures, and angry stakeholders when the time comes. This article lays out a disciplined approach to rollback strategies tailored for modern distributed databases (PostgreSQL clusters, cloud-native NoSQL, sharded NewSQL, event-sourced systems) where consistency, latency, and regulatory obligations collide.&lt;/p&gt;</description></item><item><title>Merkle Trees and Content‑Addressable Storage</title><link>https://lbenicio.dev/blog/merkle-trees-and-contentaddressable-storage/</link><pubDate>Mon, 17 Aug 2020 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/merkle-trees-and-contentaddressable-storage/</guid><description>&lt;p&gt;Hash the content, not the location—that’s the core of content‑addressable storage (CAS). Combine it with Merkle trees (or DAGs) and you get efficient verification, deduplication, and synchronization. This post connects the dots from Git to large‑scale object stores.&lt;/p&gt;
&lt;h2 id="why-merkle"&gt;Why Merkle?&lt;/h2&gt;
&lt;p&gt;Parent hashes commit to child hashes; any change percolates up. You can verify integrity by checking a root hash and a short proof path.&lt;/p&gt;
&lt;h2 id="uses"&gt;Uses&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Git commits and trees; shallow clones via missing subtrees.&lt;/li&gt;
&lt;li&gt;Package managers with integrity checks.&lt;/li&gt;
&lt;li&gt;Deduplicated backups with chunking and rolling hashes.&lt;/li&gt;
&lt;li&gt;Object stores that replicate by exchanging missing subgraphs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="practical-notes"&gt;Practical notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Hash choice (SHA‑256 vs BLAKE3) affects speed and hardware support.&lt;/li&gt;
&lt;li&gt;Chunking strategy controls dedupe granularity; rolling fingerprints (Rabin) find natural boundaries.&lt;/li&gt;
&lt;li&gt;Store metadata alongside blobs to avoid rehashing for trivial changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="1-from-trees-to-dags-modeling-versions-that-share-content"&gt;1) From trees to DAGs: modeling versions that share content&lt;/h2&gt;
&lt;p&gt;Classic Merkle trees have a fixed arity and two kinds of nodes: leaves containing hashes of fixed‑size blocks, and internal nodes containing hashes of their children. In real systems, multiple versions of content share substructure: two versions of a directory share most files, two backups share most chunks, two container images share many layers. That sharing naturally forms a directed acyclic graph (DAG) where a node can be referenced from multiple parents. The root is still a commitment to the whole, but now many roots can reference common subgraphs without duplication.&lt;/p&gt;</description></item><item><title>Tuning the Dial: Adaptive Consistency at Planet Scale</title><link>https://lbenicio.dev/blog/tuning-the-dial-adaptive-consistency-at-planet-scale/</link><pubDate>Wed, 11 Mar 2020 14:05:00 +0000</pubDate><guid>https://lbenicio.dev/blog/tuning-the-dial-adaptive-consistency-at-planet-scale/</guid><description>&lt;p&gt;At 2:17 a.m. UTC, a partner bank in Singapore called our incident bridge. A fund transfer appeared twice in their ledger. The culprit: a replication lag spike between Singapore and Frankfurt had stretched past our standard safety buffers. Historically we would have halted writes across the fleet, cutting availability to protect consistency. Instead, our adaptive consistency layer dialed a region-specific policy: Singapore moved from &amp;ldquo;read-after-write&amp;rdquo; to &amp;ldquo;read-your-writes&amp;rdquo; guarantees, while Frankfurt raised its commit quorum. The double posting self-corrected before social media noticed. No downtime, no irreversible loss—just a story about how we learned to treat consistency as a spectrum rather than a binary.&lt;/p&gt;</description></item><item><title>When Data Centers Learned to Sleep: Energy-Aware Scheduling in Practice</title><link>https://lbenicio.dev/blog/when-data-centers-learned-to-sleep-energy-aware-scheduling-in-practice/</link><pubDate>Fri, 19 Jul 2019 09:30:00 +0000</pubDate><guid>https://lbenicio.dev/blog/when-data-centers-learned-to-sleep-energy-aware-scheduling-in-practice/</guid><description>&lt;p&gt;The first time we let a data center &amp;ldquo;sleep&amp;rdquo; during daylight hours felt reckless. Customers trusted us with near-infinite elasticity. Flipping servers into deep power states to save energy sounded like penny-pinching, not engineering. Yet the math was undeniable: idling a hyperscale fleet burned as much electricity as a mid-size city. This post tells the story of how we evolved from skepticism to confidence, building energy-aware scheduling systems that keep promises while honoring planetary limits.&lt;/p&gt;</description></item><item><title>Speculative Prefetchers: Designing Memory Systems That Read the Future</title><link>https://lbenicio.dev/blog/speculative-prefetchers-designing-memory-systems-that-read-the-future/</link><pubDate>Thu, 14 Feb 2019 10:00:00 +0000</pubDate><guid>https://lbenicio.dev/blog/speculative-prefetchers-designing-memory-systems-that-read-the-future/</guid><description>&lt;p&gt;At 2:17 a.m., the on-call performance engineer watches another alert crawl across the dashboard. The new machine image promised higher throughput for a latency-sensitive analytics service, yet caches still thrash whenever end-of-day reconciliation jobs arrive. Each job walks a sparsely linked graph of customer transactions, and the CPU spends more time waiting on memory than executing instructions. &amp;ldquo;If only the hardware could guess where the program was going next,&amp;rdquo; she sighs. That daydream is the seed of speculative prefetching—the art of reading tomorrow’s memory today.&lt;/p&gt;</description></item><item><title>A Mathematical Theory of Communication</title><link>https://lbenicio.dev/reading/a-mathematical-theory-of-communication/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/a-mathematical-theory-of-communication/</guid><description>&lt;p&gt;Foundational paper that created information theory, introducing entropy and channel capacity.&lt;/p&gt;</description></item><item><title>Algorithm Design</title><link>https://lbenicio.dev/reading/algorithm-design/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/algorithm-design/</guid><description>&lt;p&gt;A practical approach to algorithm design with emphasis on design paradigms and proofs of correctness.&lt;/p&gt;</description></item><item><title>Algorithms</title><link>https://lbenicio.dev/reading/algorithms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/algorithms/</guid><description>&lt;p&gt;An accessible algorithms textbook with strong intuition and mathematical clarity.&lt;/p&gt;</description></item><item><title>Algorithms (4th ed.)</title><link>https://lbenicio.dev/reading/algorithms-4th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/algorithms-4th-ed./</guid><description>&lt;p&gt;A comprehensive introduction to algorithms with Java implementations and extensive online resources.&lt;/p&gt;</description></item><item><title>Artificial Intelligence: A Modern Approach (4th ed.)</title><link>https://lbenicio.dev/reading/artificial-intelligence-a-modern-approach-4th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/artificial-intelligence-a-modern-approach-4th-ed./</guid><description>&lt;p&gt;Definitive textbook on AI covering search, knowledge, planning, learning, and applications.&lt;/p&gt;</description></item><item><title>Artificial Intelligence: A Modern Approach (4th ed.)</title><link>https://lbenicio.dev/reading/artificial-intelligence-a-modern-approach-4th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/artificial-intelligence-a-modern-approach-4th-ed./</guid><description>&lt;p&gt;Definitive AI textbook covering search, knowledge, planning, learning, and applications.&lt;/p&gt;</description></item><item><title>Causality: Models, Reasoning, and Inference (2nd ed.)</title><link>https://lbenicio.dev/reading/causality-models-reasoning-and-inference-2nd-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/causality-models-reasoning-and-inference-2nd-ed./</guid><description>&lt;p&gt;A rigorous treatment of causal inference and graphical models.&lt;/p&gt;</description></item><item><title>Clean Code: A Handbook of Agile Software Craftsmanship</title><link>https://lbenicio.dev/reading/clean-code-a-handbook-of-agile-software-craftsmanship/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/clean-code-a-handbook-of-agile-software-craftsmanship/</guid><description>&lt;p&gt;Guidelines and principles for writing readable, maintainable, and testable code.&lt;/p&gt;</description></item><item><title>Compilers: Principles, Techniques, and Tools (2nd ed.)</title><link>https://lbenicio.dev/reading/compilers-principles-techniques-and-tools-2nd-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/compilers-principles-techniques-and-tools-2nd-ed./</guid><description>&lt;p&gt;The classic &amp;ldquo;Dragon Book&amp;rdquo; covering the theory and practice of compiler construction.&lt;/p&gt;</description></item><item><title>Computer Architecture: A Quantitative Approach (6th ed.)</title><link>https://lbenicio.dev/reading/computer-architecture-a-quantitative-approach-6th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/computer-architecture-a-quantitative-approach-6th-ed./</guid><description>&lt;p&gt;Definitive textbook on modern computer architecture and performance analysis.&lt;/p&gt;</description></item><item><title>Computer Architecture: A Quantitative Approach (6th ed.)</title><link>https://lbenicio.dev/reading/computer-architecture-a-quantitative-approach-6th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/computer-architecture-a-quantitative-approach-6th-ed./</guid><description>&lt;p&gt;An in-depth treatment of modern processor and system design with quantitative evaluation.&lt;/p&gt;</description></item><item><title>Computer Networking: A Top-Down Approach (7th ed.)</title><link>https://lbenicio.dev/reading/computer-networking-a-top-down-approach-7th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/computer-networking-a-top-down-approach-7th-ed./</guid><description>&lt;p&gt;A top-down exploration of computer networking from applications to physical layer.&lt;/p&gt;</description></item><item><title>Computer Networks (5th ed.)</title><link>https://lbenicio.dev/reading/computer-networks-5th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/computer-networks-5th-ed./</guid><description>&lt;p&gt;Classic text covering network layers, protocols, and architectures.&lt;/p&gt;</description></item><item><title>Computer Systems: A Programmer's Perspective (3rd ed.)</title><link>https://lbenicio.dev/reading/computer-systems-a-programmers-perspective-3rd-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/computer-systems-a-programmers-perspective-3rd-ed./</guid><description>&lt;p&gt;Bridges programming and computer architecture with a focus on performance and systems.&lt;/p&gt;</description></item><item><title>Computer Systems: A Programmer's Perspective (3rd ed.)</title><link>https://lbenicio.dev/reading/computer-systems-a-programmers-perspective-3rd-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/computer-systems-a-programmers-perspective-3rd-ed./</guid><description>&lt;p&gt;Bridges the gap between programming and computer architecture with a focus on performance and systems.&lt;/p&gt;</description></item><item><title>Consul: Rails Web Application Scalability and Performance Improvements with Distributed Computation</title><link>https://lbenicio.dev/publications/consul-rails-web-application-scalability-and-performance-improvements-with-distributed-computation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/publications/consul-rails-web-application-scalability-and-performance-improvements-with-distributed-computation/</guid><description>&lt;p&gt;Undergraduate monograph at IME-USP (Computer Science). Focuses on improving scalability and performance of a Rails application using distributed computation.&lt;/p&gt;</description></item><item><title>Contact</title><link>https://lbenicio.dev/single/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/single/contact/</guid><description>&lt;p&gt;Get in touch for questions, collaboration, or opportunities. You can also use the main contact page.&lt;/p&gt;
&lt;h2 id="email"&gt;Email&lt;/h2&gt;
&lt;p&gt;You can reach me directly at &lt;a href="mailto:hi@lbenicio.dev"&gt;hi@lbenicio.dev&lt;/a&gt; or use the form below:&lt;/p&gt;
&lt;div class="mt-6"&gt;
 
 
 &lt;section class="w-full max-w-none"&gt;
 &lt;h2 class="sr-only"&gt;Contact and PGP&lt;/h2&gt;
 &lt;div class="grid grid-cols-1 md:grid-cols-2 gap-6 w-full"&gt;
 &lt;div&gt;
 &lt;h2 class="text-2xl font-semibold tracking-tight"&gt;Send a Message&lt;/h2&gt;
 &lt;p class="text-sm text-muted-foreground"&gt;Fill out the form and I will reply as soon as possible. Encrypted messages supported via OpenPGP.&lt;/p&gt;
 &lt;form data-contact-form class="space-y-4 mt-4 w-full" method="post" action="#" onsubmit="return submitContact(event)"&gt;
 
 &lt;div style="position:absolute;left:-9999px;top:auto;width:1px;height:1px;overflow:hidden;" aria-hidden="true"&gt;
 &lt;label for="hp"&gt;Leave this field empty&lt;/label&gt;
 &lt;input data-hp name="hp" type="text" autocomplete="off" tabindex="-1" /&gt;
 &lt;/div&gt;
 &lt;div class="grid grid-cols-2 gap-3"&gt;
 &lt;div&gt;
 &lt;label for="first_name" class="block text-sm font-medium mb-1"&gt;First name&lt;/label&gt;
 &lt;input data-first-name name="first_name" type="text" required class="w-full rounded-md border bg-background px-3 py-2 text-sm focus:outline-none focus:ring focus:ring-ring" /&gt;
 &lt;/div&gt;
 &lt;div&gt;
 &lt;label for="last_name" class="block text-sm font-medium mb-1"&gt;Last name&lt;/label&gt;
 &lt;input data-last-name name="last_name" type="text" required class="w-full rounded-md border bg-background px-3 py-2 text-sm focus:outline-none focus:ring focus:ring-ring" /&gt;
 &lt;/div&gt;
 &lt;/div&gt;
 &lt;div&gt;
 &lt;label for="email" class="block text-sm font-medium mb-1"&gt;Email&lt;/label&gt;
 &lt;input data-email name="email" type="email" required class="w-full rounded-md border bg-background px-3 py-2 text-sm focus:outline-none focus:ring focus:ring-ring" /&gt;
 &lt;/div&gt;
 &lt;div&gt;
 &lt;label for="subject" class="block text-sm font-medium mb-1"&gt;Subject&lt;/label&gt;
 &lt;input data-subject name="subject" type="text" class="w-full rounded-md border bg-background px-3 py-2 text-sm focus:outline-none focus:ring focus:ring-ring" /&gt;
 &lt;/div&gt;
 &lt;div&gt;
 &lt;label for="text" class="block text-sm font-medium mb-1"&gt;Message&lt;/label&gt;
 &lt;textarea data-text name="text" rows="5" required class="w-full rounded-md border bg-background px-3 py-2 text-sm focus:outline-none focus:ring focus:ring-ring"&gt;&lt;/textarea&gt;
 &lt;/div&gt;
 &lt;div class="flex items-center gap-3"&gt;
 &lt;label class="inline-flex items-center gap-2 text-sm cursor-pointer"&gt;
 &lt;input data-encrypt type="checkbox" class="h-4 w-4" /&gt;
 Encrypt with PGP
 &lt;/label&gt;
 &lt;span data-pgp-error class="text-xs text-red-600 hidden"&gt;&lt;/span&gt;
 &lt;/div&gt;
 &lt;div class="flex items-center gap-3"&gt;
 &lt;button data-contact-submit type="submit" class="px-4 py-2 text-sm border rounded hover:bg-muted"&gt;Send Message&lt;/button&gt;
 &lt;div data-contact-loading class="hidden text-sm text-muted-foreground"&gt;Sending…&lt;/div&gt;
 &lt;/div&gt;
 &lt;/form&gt;
 &lt;div data-contact-toast class="hidden max-w-none rounded-md px-4 py-3 shadow-lg mt-4" role="status" aria-live="polite"&gt;&lt;/div&gt;

 &lt;script id="contact-config" type="application/json"&gt;
 
 "{\"authToken\":\"XQKURG66oM4coZZDRQi990QL5iLXNjxA3X1z8GPshFHUwV5ldy8oPZ6pBfeo3qIRe161P330zhI9umgX/eVQNX2Rge7VenGatdKLNQ4qRELAu3I0WPGc+bx9Lx5izIWVifLydmcnzyVKumt36eIZiEhNWAdzkHH4ho6fxzql+JpOqKkOBGUYm77VAHkbfXBwdJheXy4phi9ltZEXEcC7X4fzfev8WsAidM2XKVPA7hyIBrTrDTKFkZ1UhTUcOpmso/WFOGAtzEv0SUEuR4r5JZ8yFmKWqEJL3mOzjIBR2hrRaPw8lNGOJWjMjtK5t22fLLwEpNsW7HBaWW9MCrTT18050m6Sffoy89r1sQt3I0Fc5HruxceCTUzDKGf3KQHiCL/w4lNc4kULaVBY\",\"email\":\"hi@lbenicio.dev\",\"name\":\"Leonardo Benicio\",\"postUrl\":\"https://formbold.com/s/oJ4Vw\"}"
 &lt;/script&gt;

 &lt;script&gt;
 (function(){
 
 const contactConfigEl = document.getElementById('contact-config');
 const contactConfig = contactConfigEl ? JSON.parse(contactConfigEl.textContent || '{}') : {};
 const contactPostUrl = contactConfig.postUrl || '';
 const contactAuthToken = contactConfig.authToken || '';

 
 function qs(sel){ return document.querySelector(sel); }
 function getVal(sel){ const el = qs(sel); return el ? el.value.trim() : ''; }
 function getChecked(sel){ const el = qs(sel); return el ? el.checked : false; }

 
 try {
 const raw = localStorage.getItem('contact:unsent');
 if (raw) {
 const obj = JSON.parse(raw);
 if (obj.first_name) qs('[data-first-name]').value = obj.first_name;
 if (obj.last_name) qs('[data-last-name]').value = obj.last_name;
 if (obj.email) qs('[data-email]').value = obj.email;
 if (obj.subject) qs('[data-subject]').value = obj.subject;
 if (obj.text) qs('[data-text]').value = obj.text;
 if (obj.encrypt) qs('[data-encrypt]').checked = obj.encrypt;
 }
 } catch(e){}

 
 ['[data-hp]','[data-first-name]','[data-last-name]','[data-email]','[data-subject]','[data-text]','[data-encrypt]'].forEach(sel=&gt;{
 const el = qs(sel); if (!el) return; el.addEventListener('input', ()=&gt;{
 try{
 const obj = {
 hp: getVal('[data-hp]') || '',
 first_name: getVal('[data-first-name]') || '',
 last_name: getVal('[data-last-name]') || '',
 email: getVal('[data-email]') || '',
 subject: getVal('[data-subject]') || '',
 text: getVal('[data-text]') || '',
 encrypt: getChecked('[data-encrypt]') || false,
 };
 localStorage.setItem('contact:unsent', JSON.stringify(obj));
 }catch(e){}
 });
 });

 function setLoading(on){ const btn = qs('[data-contact-submit]'); const loading = qs('[data-contact-loading]'); if(!btn||!loading) return; if(on){ btn.disabled=true; loading.classList.remove('hidden'); } else { btn.disabled=false; loading.classList.add('hidden'); } }

 function showToast(msg, withRetry=false){ const t = qs('[data-contact-toast]'); if(!t) return; t.innerHTML=''; t.classList.remove('hidden'); t.style.background = withRetry ? '#fff7ed' : '#ecfeff'; t.style.color = withRetry ? '#92400e' : '#065f46'; const span = document.createElement('span'); span.innerText = msg; t.appendChild(span);
 if (withRetry){ const retryBtn = document.createElement('button'); retryBtn.type='button'; retryBtn.className='ml-3 px-2 py-1 text-sm border rounded'; retryBtn.innerText='Retry'; retryBtn.addEventListener('click', ()=&gt;{ const raw = localStorage.getItem('contact:lastPayload'); if(!raw) return; try{ const p = JSON.parse(raw); if(p.first_name) qs('[data-first-name]').value = p.first_name; if(p.last_name) qs('[data-last-name]').value = p.last_name; if(p.email) qs('[data-email]').value = p.email; if(p.subject) qs('[data-subject]').value = p.subject; if(p.text) qs('[data-text]').value = p.text; qs('[data-encrypt]').checked = !!p.encrypt; submitContact(null, true); }catch(e){ console.warn('retry parse failed', e); } }); t.appendChild(retryBtn); }
 const close = document.createElement('button'); close.type='button'; close.className='ml-2 px-2'; close.innerText='×'; close.addEventListener('click', ()=&gt; t.classList.add('hidden')); t.appendChild(close); if(!withRetry) setTimeout(()=&gt; t.classList.add('hidden'), 4000);
 }

 async function submitContact(e, retrying=false){ if(e &amp;&amp; e.preventDefault) e.preventDefault();
 const hp = getVal('[data-hp]');
 const firstName = getVal('[data-first-name]');
 const lastName = getVal('[data-last-name]');
 const email = getVal('[data-email]');
 const subject = getVal('[data-subject]');
 const text = getVal('[data-text]');
 const encrypt = getChecked('[data-encrypt]');
 const payloadMeta = { hp, first_name: firstName, last_name: lastName, email, subject, text, encrypt };
 try{ localStorage.setItem('contact:unsent', JSON.stringify({ first_name:firstName,last_name:lastName,email,subject,text,encrypt })); }catch(e){}
 try{ localStorage.setItem('contact:lastPayload', JSON.stringify(payloadMeta)); }catch(e){}

 if (!contactPostUrl){ 
 const body = `Name: ${firstName} ${lastName}\nEmail: ${email}\n\nMessage:\n${text}`;
 if (encrypt) { try { await navigator.clipboard.writeText(body); } catch(e){} showToast('PGP: message copied to clipboard. Use the public key on the right.'); }
 const mailto = 'mailto:' + encodeURIComponent(contactConfig.email) + '?subject=' + encodeURIComponent('Contact from ' + (firstName||'')) + '&amp;body=' + encodeURIComponent(body);
 try{ localStorage.removeItem('contact:unsent'); }catch(e){}
 window.location.href = mailto; return false;
 }

 setLoading(true);
 try{
 
 let bodyToSend = payloadMeta;
 if (encrypt){
 try{ const shim = await import('/assets/js/openpgp-shim.mjs'); await shim.ensureOpenPGP(); }catch{ console.warn('Failed to load OpenPGP shim'); const pgpErr = qs('[data-pgp-error]'); if(pgpErr){ pgpErr.classList.remove('hidden'); pgpErr.textContent='Failed to load encryption library'; } }
 try{
 const keyRes = await fetch('\/static\/pgp\/public.asc', { cache: 'force-cache' }); if(!keyRes.ok) throw new Error('PGP key load failed'); const publicArmored = await keyRes.text(); const pubKey = await openpgp.readKey({ armoredKey: publicArmored }); const clear = `Name: ${firstName} ${lastName}\nEmail: ${email}\n\nMessage:\n${text}`; const encrypted = await openpgp.encrypt({ message: await openpgp.createMessage({ text: clear }), encryptionKeys: pubKey }); bodyToSend = { hp, first_name: firstName, last_name: lastName, email, text: encrypted, encrypted: true };
 }catch(err){ console.warn('Encryption failed', err); const pgpErr = qs('[data-pgp-error]'); if(pgpErr){ pgpErr.classList.remove('hidden'); pgpErr.textContent='Encryption failed — sending unencrypted'; } bodyToSend = payloadMeta; }
 }

 const data = {
 from: { email: email, name: `${firstName} ${lastName}`.trim() },
 subject: subject,
 text: encrypt ? (bodyToSend.text || '') : text,
 };

 const headers = { 'Content-Type': 'application/json' };
 if (contactAuthToken &amp;&amp; String(contactAuthToken).length) headers['Authorization'] = String(contactAuthToken);
 const res = await fetch(contactPostUrl, { method: 'POST', headers, body: JSON.stringify(data) });
 setLoading(false);
 if (res.ok){ showToast('Message sent — thank you!'); const form = qs('[data-contact-form]'); if(form) form.reset(); try{ localStorage.removeItem('contact:unsent'); localStorage.removeItem('contact:lastPayload'); }catch(e){} return false; }
 const txt = await res.text(); showToast('Submission failed: ' + (txt || res.statusText), true); return false;
 }catch(err){ setLoading(false); showToast('Submission failed: ' + (err &amp;&amp; err.message ? err.message : 'network error'), true); return false; }
 }

 
 const formEl = qs('[data-contact-form]'); if(formEl) formEl.addEventListener('submit', submitContact);
 })();
 &lt;/script&gt;
 &lt;/div&gt;

 &lt;div&gt;
 &lt;h2 class="text-2xl font-semibold tracking-tight"&gt;PGP Public Key&lt;/h2&gt;
 &lt;p class="text-sm text-muted-foreground"&gt;Import this key to send encrypted email. Copy or download the armored block.&lt;/p&gt;</description></item><item><title>Database System Concepts (7th ed.)</title><link>https://lbenicio.dev/reading/database-system-concepts-7th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/database-system-concepts-7th-ed./</guid><description>&lt;p&gt;Foundational text on database design, SQL, normalization, transactions, and architectures.&lt;/p&gt;</description></item><item><title>Database System Concepts (7th ed.)</title><link>https://lbenicio.dev/reading/database-system-concepts-7th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/database-system-concepts-7th-ed./</guid><description>&lt;p&gt;Foundational text on database design, SQL, transactions, and architectures.&lt;/p&gt;</description></item><item><title>Deep Learning</title><link>https://lbenicio.dev/reading/deep-learning/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/deep-learning/</guid><description>&lt;p&gt;Comprehensive textbook on deep learning theory and practice.&lt;/p&gt;</description></item><item><title>Design Patterns: Elements of Reusable Object-Oriented Software</title><link>https://lbenicio.dev/reading/design-patterns-elements-of-reusable-object-oriented-software/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/design-patterns-elements-of-reusable-object-oriented-software/</guid><description>&lt;p&gt;Seminal catalog of software design patterns for building flexible, reusable object-oriented systems.&lt;/p&gt;</description></item><item><title>Distributed Systems: Principles and Paradigms (2nd ed.)</title><link>https://lbenicio.dev/reading/distributed-systems-principles-and-paradigms-2nd-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/distributed-systems-principles-and-paradigms-2nd-ed./</guid><description>&lt;p&gt;Foundational coverage of distributed system models, communication, consistency, and coordination.&lt;/p&gt;</description></item><item><title>Improving the Scalability and Performance of a Rails Application: A Case Study with Consul</title><link>https://lbenicio.dev/publications/improving-the-scalability-and-performance-of-a-rails-application-a-case-study-with-consul/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/publications/improving-the-scalability-and-performance-of-a-rails-application-a-case-study-with-consul/</guid><description>&lt;p&gt;Case study evaluating scalability and performance improvements of a Ruby on Rails application using Consul.&lt;/p&gt;</description></item><item><title>Introduction to Algorithms (3rd ed.)</title><link>https://lbenicio.dev/reading/introduction-to-algorithms-3rd-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/introduction-to-algorithms-3rd-ed./</guid><description>&lt;p&gt;A comprehensive and rigorous textbook covering algorithms and data structures, widely used in CS curricula.&lt;/p&gt;</description></item><item><title>Introduction to Automata Theory, Languages, and Computation (3rd ed.)</title><link>https://lbenicio.dev/reading/introduction-to-automata-theory-languages-and-computation-3rd-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/introduction-to-automata-theory-languages-and-computation-3rd-ed./</guid><description>&lt;p&gt;Classic text on automata, formal languages, Turing machines, and computational complexity.&lt;/p&gt;</description></item><item><title>Introduction to Parallel Computing (2nd ed.)</title><link>https://lbenicio.dev/reading/introduction-to-parallel-computing-2nd-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/introduction-to-parallel-computing-2nd-ed./</guid><description>&lt;p&gt;A comprehensive textbook on models, algorithms, and performance analysis for parallel computing.&lt;/p&gt;</description></item><item><title>Introduction to the Theory of Computation (3rd ed.)</title><link>https://lbenicio.dev/reading/introduction-to-the-theory-of-computation-3rd-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/introduction-to-the-theory-of-computation-3rd-ed./</guid><description>&lt;p&gt;Accessible introduction to automata theory, computability, and complexity.&lt;/p&gt;</description></item><item><title>Mining of Massive Datasets (3rd ed.)</title><link>https://lbenicio.dev/reading/mining-of-massive-datasets-3rd-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/mining-of-massive-datasets-3rd-ed./</guid><description>&lt;p&gt;Core techniques for large-scale data mining and analysis.&lt;/p&gt;</description></item><item><title>Modern Operating Systems (4th ed.)</title><link>https://lbenicio.dev/reading/modern-operating-systems-4th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/modern-operating-systems-4th-ed./</guid><description>&lt;h2 id="a-comprehensive-os-textbook-covering-processes-memory-management-file-systems-and-security"&gt;A comprehensive OS textbook covering processes, memory management, file systems, and security&lt;/h2&gt;
&lt;p&gt;A comprehensive OS textbook covering processes, memory management, file systems, and security.&lt;/p&gt;</description></item><item><title>Newsletter</title><link>https://lbenicio.dev/single/newsletter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/single/newsletter/</guid><description>&lt;p&gt;Subscribe to my monthly newsletter where I share updates, essays, and selected readings.&lt;/p&gt;
&lt;div class="mt-6"&gt;
 &lt;p&gt;&lt;/p&gt;
 &lt;form action="https://mail.4686189.xyz/subscription/form" method="post" class="mt-4 space-y-2"&gt;
 &lt;input type="text" name="name" placeholder="Your name (optional)" class="w-full rounded-md border px-3 py-2" /&gt;
 &lt;input type="email" name="email" placeholder="you@example.com" required class="w-full rounded-md border px-3 py-2" /&gt;
 
 
 &lt;input type="checkbox" id="d6910" data-newsletter-checkbox-id="d6910" name="l" value="1d69100ad-12e9-4eed-a951-393a39fd41a6" hidden checked /&gt;
 
 &lt;div class="flex gap-2"&gt;
 &lt;button type="submit" class="rounded-md px-4 py-2 bg-accent text-primary"&gt;Subscribe&lt;/button&gt;
 &lt;a href="https://lbenicio.dev/" class="rounded-md px-4 py-2 border text-primary"&gt;Cancel&lt;/a&gt;
 &lt;/div&gt;
 &lt;/form&gt;
&lt;/div&gt;</description></item><item><title>On Computable Numbers, with an Application to the Entscheidungsproblem</title><link>https://lbenicio.dev/reading/on-computable-numbers-with-an-application-to-the-entscheidungsproblem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/on-computable-numbers-with-an-application-to-the-entscheidungsproblem/</guid><description>&lt;p&gt;Seminal paper introducing the concept of Turing machines and computability.&lt;/p&gt;</description></item><item><title>OpenMP Application Programming Interface, Version 5.2</title><link>https://lbenicio.dev/reading/openmp-application-programming-interface-version-5.2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/openmp-application-programming-interface-version-5.2/</guid><description>&lt;p&gt;Official OpenMP specification with the latest directives, memory model, and examples.&lt;/p&gt;</description></item><item><title>Operating System Concepts (9th ed.)</title><link>https://lbenicio.dev/reading/operating-system-concepts-9th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/operating-system-concepts-9th-ed./</guid><description>&lt;p&gt;Foundations of operating systems including processes, threads, scheduling, memory, and file systems.&lt;/p&gt;</description></item><item><title>Operating Systems: Three Easy Pieces</title><link>https://lbenicio.dev/reading/operating-systems-three-easy-pieces/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/operating-systems-three-easy-pieces/</guid><description>&lt;p&gt;A free, modular OS textbook with hands-on projects and clear explanations.&lt;/p&gt;</description></item><item><title>Optimizing HPC Deployment: Enhancing Accessibility and Efficiency through the OMPC Framework</title><link>https://lbenicio.dev/publications/optimizing-hpc-deployment-enhancing-accessibility-and-efficiency-through-the-ompc-framework/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/publications/optimizing-hpc-deployment-enhancing-accessibility-and-efficiency-through-the-ompc-framework/</guid><description>&lt;p&gt;Paper presented at ERAD-SP 2024. Proposes the OMPC framework to improve accessibility and efficiency for HPC deployments.&lt;/p&gt;</description></item><item><title>Parallel Programming in C with MPI and OpenMP</title><link>https://lbenicio.dev/reading/parallel-programming-in-c-with-mpi-and-openmp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/parallel-programming-in-c-with-mpi-and-openmp/</guid><description>&lt;p&gt;Introductory yet practical coverage of parallel programming in C using both MPI and OpenMP.&lt;/p&gt;</description></item><item><title>Patterns for Parallel Programming</title><link>https://lbenicio.dev/reading/patterns-for-parallel-programming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/patterns-for-parallel-programming/</guid><description>&lt;p&gt;Pattern-based approach to structuring parallel programs across shared and distributed memory.&lt;/p&gt;</description></item><item><title>Principles of Database and Knowledge-Base Systems</title><link>https://lbenicio.dev/reading/principles-of-database-and-knowledge-base-systems/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/principles-of-database-and-knowledge-base-systems/</guid><description>&lt;p&gt;Foundational theory for databases and knowledge-base systems.&lt;/p&gt;</description></item><item><title>Principles of Program Analysis</title><link>https://lbenicio.dev/reading/principles-of-program-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/principles-of-program-analysis/</guid><description>&lt;p&gt;Formal foundations of program analysis including data-flow and abstract interpretation.&lt;/p&gt;</description></item><item><title>Probabilistic Reasoning in Intelligent Systems</title><link>https://lbenicio.dev/reading/probabilistic-reasoning-in-intelligent-systems/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/probabilistic-reasoning-in-intelligent-systems/</guid><description>&lt;p&gt;A seminal work on Bayesian networks and probabilistic reasoning.&lt;/p&gt;</description></item><item><title>Security Engineering (3rd ed.)</title><link>https://lbenicio.dev/reading/security-engineering-3rd-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/security-engineering-3rd-ed./</guid><description>&lt;p&gt;An encyclopedic treatment of security engineering principles, attacks, and defenses.&lt;/p&gt;</description></item><item><title>Socials</title><link>https://lbenicio.dev/single/socials/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/single/socials/</guid><description>&lt;ul&gt;
&lt;li&gt;






&lt;svg class="h-4 w-4 inline-block align-middle mr-2" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" aria-hidden="true" role="img"&gt;&lt;title&gt;Twitter&lt;/title&gt;&lt;path d="M23 3a10.9 10.9 0 01-3.14 1.53A4.48 4.48 0 0016.5 3c-2.5 0-4.5 2.24-4 4.7A12.94 12.94 0 013 4s-4 9 5 13a13 13 0 01-8 2c9 5 20 0 20-11.5a4.5 4.5 0 00-.08-.83A7.72 7.72 0 0023 3z"/&gt;&lt;/svg&gt; Twitter: &lt;a href="https://twitter.com/lbenicio_"&gt;lbenicio_&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;






&lt;svg class="h-4 w-4 inline-block align-middle mr-2" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" role="img"&gt;&lt;title&gt;LinkedIn&lt;/title&gt;&lt;path d="M4.98 3.5C4.98 4.88 3.88 6 2.5 6S0 4.88 0 3.5 1.12 1 2.5 1 4.98 2.12 4.98 3.5zM0 8h5v16H0V8zm7 0h4.78v2.18h.07c.67-1.27 2.31-2.61 4.75-2.61 5.08 0 6.02 3.35 6.02 7.71V24H17v-8.03c0-1.92-.03-4.39-2.68-4.39-2.69 0-3.1 2.1-3.1 4.26V24H7V8z"/&gt;&lt;/svg&gt; LinkedIn: &lt;a href="https://www.linkedin.com/in/leonardo-benicio"&gt;leonardo-benicio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;






&lt;svg class="h-4 w-4 inline-block align-middle mr-2" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" role="img"&gt;&lt;title&gt;GitHub&lt;/title&gt;&lt;path d="M12 .5C5.65.5.5 5.76.5 12.19c0 5.13 3.44 9.47 8.2 11.01.6.11.82-.26.82-.58 0-.29-.01-1.05-.02-2.06-3.34.74-4.04-1.6-4.04-1.6-.55-1.4-1.34-1.77-1.34-1.77-1.09-.75.08-.74.08-.74 1.2.09 1.84 1.24 1.84 1.24 1.07 1.85 2.8 1.32 3.48 1.01.11-.79.42-1.32.77-1.62-2.67-.31-5.47-1.36-5.47-6.05 0-1.34.47-2.43 1.23-3.29-.12-.31-.54-1.56.12-3.24 0 0 1.01-.33 3.3 1.25a11.26 11.26 0 016 0c2.28-1.58 3.29-1.25 3.29-1.25.67 1.68.25 2.93.12 3.24.77.86 1.23 1.95 1.23 3.29 0 4.7-2.81 5.73-5.48 6.03.43.38.82 1.12.82 2.25 0 1.62-.01 2.92-.01 3.32 0 .32.21.69.83.57A11.73 11.73 0 0023.5 12.19C23.5 5.76 18.35.5 12 .5z"/&gt;&lt;/svg&gt; GitHub: &lt;a href="https://github.com/lbenicio"&gt;lbenicio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;






&lt;svg class="h-4 w-4 inline-block align-middle mr-2" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" role="img"&gt;&lt;title&gt;Reddit&lt;/title&gt;&lt;path d="M22 12.4c0-1.9-1.5-3.4-3.4-3.4-.9 0-1.7.3-2.3.8-1.7-1.1-3.8-1.8-6.1-1.9l1.1-4.9 2.9.7c0 1.1.9 2 2 2s2-.9 2-2-0.9-2-2-2-2 .9-2 2c0 .2.02.4.06.6l-3.7-.9c-.2-.05-.4.06-.46.28L9.2 8.6C5.6 9.1 2.6 12.2 2.6 15.9c0 3.7 3 6.7 6.7 6.7 2.5 0 4.6-1.4 5.7-3.4 1.2.6 2.7.9 4.2.9 2.6 0 4.6-1.7 4.6-3.9 0-1.5-1-2.7-2.7-3.1.8-.4 1.3-1.1 1.3-1.9z"/&gt;&lt;/svg&gt; Reddit: &lt;a href="https://www.reddit.com/user/lbenicio"&gt;u/lbenicio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;






&lt;svg class="h-4 w-4 inline-block align-middle mr-2" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="1.5" aria-hidden="true" role="img"&gt;&lt;title&gt;Email&lt;/title&gt;&lt;path d="M3 8.5l9 6 9-6" stroke-linecap="round" stroke-linejoin="round"/&gt;&lt;path d="M21 6H3v12h18V6z" stroke-linecap="round" stroke-linejoin="round"/&gt;&lt;/svg&gt; Email: &lt;a href="mailto:hi@lbenicio.dev"&gt;hi@lbenicio.dev&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;






&lt;svg class="h-4 w-4 inline-block align-middle mr-2" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" role="img"&gt;&lt;title&gt;Instagram&lt;/title&gt;&lt;path d="M7 2h10a5 5 0 015 5v10a5 5 0 01-5 5H7a5 5 0 01-5-5V7a5 5 0 015-5zm5 6.5A4.5 4.5 0 1016.5 13 4.5 4.5 0 0012 8.5zM18 6.2a1.2 1.2 0 11-1.2-1.2A1.2 1.2 0 0118 6.2z"/&gt;&lt;/svg&gt; Instagram: &lt;a href="https://www.instagram.com/lbenicio_"&gt;lbenicio_&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;






&lt;svg class="h-4 w-4 inline-block align-middle mr-2" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" role="img"&gt;&lt;title&gt;Buyme a coffee&lt;/title&gt;&lt;path d="M18 8h1a3 3 0 010 6h-1v1a3 3 0 01-3 3H8a3 3 0 01-3-3V8a3 3 0 013-3h7a3 3 0 013 3zM6 8v8"/&gt;&lt;/svg&gt; Buyme a coffee: &lt;a href="https://www.buymeacoffee.com/lbenicio"&gt;buymeacoffee.com/lbenicio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;






&lt;svg class="h-4 w-4 inline-block align-middle mr-2" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" role="img"&gt;&lt;title&gt;BlueSky&lt;/title&gt;&lt;path d="M12 2C6.5 2 2 6.5 2 12s4.5 10 10 10 10-4.5 10-10S17.5 2 12 2zm3.8 15.2c-.4.5-1.2.7-2 .7-2.4 0-4.1-1.9-4.1-4.8 0-2.3 1.2-4.1 3.2-4.1.9 0 1.7.4 2.1 1l-.4.5c-.4-.3-.8-.5-1.4-.5-1.2 0-2.2 1.1-2.2 2.9 0 2 1 3.2 2.4 3.2.8 0 1.4-.2 1.9-.7l.5.7z"/&gt;&lt;/svg&gt; BlueSky: &lt;a href="https://bsky.app/profile/lbenicio.bsky.social"&gt;lbenicio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;






&lt;svg class="h-4 w-4 inline-block align-middle mr-2" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" role="img"&gt;&lt;title&gt;YouTube&lt;/title&gt;&lt;path d="M23.5 6.2a3 3 0 00-2.1-2.1C19.7 3.5 12 3.5 12 3.5s-7.7 0-9.4.6A3 3 0 000 6.2 31.7 31.7 0 000 12a31.7 31.7 0 000 5.8 3 3 0 002.1 2.1c1.7.6 9.4.6 9.4.6s7.7 0 9.4-.6a3 3 0 002.1-2.1A31.7 31.7 0 0024 12a31.7 31.7 0 00-.5-5.8zM9.8 15.5v-7l6 3.5-6 3.5z"/&gt;&lt;/svg&gt; YouTube: &lt;a href="https://www.youtube.com/channel/@lbenicio"&gt;channel/@lbenicio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;






&lt;svg class="h-4 w-4 inline-block align-middle mr-2" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor" aria-hidden="true" role="img"&gt;&lt;title&gt;Mastodon&lt;/title&gt;&lt;path d="M12 2C7 2 3 6 3 11c0 4.2 3 7.5 7 8.5v2.5c0 .3.2.5.5.5.2 0 .4-.1.5-.2l2.7-2.7c1.3.2 2.6.2 4-.1 1.6-.3 2.9-1.3 3.5-2.8.4-1.1.4-2.2.2-3.2-.5-2.6-2.7-4.7-5.2-5.6C16.7 3 14.5 2 12 2z"/&gt;&lt;/svg&gt; Mastodon: &lt;a href="https://mastodon.social/@lbenicio"&gt;@lbenicio&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description></item><item><title>Software Engineering (10th ed.)</title><link>https://lbenicio.dev/reading/software-engineering-10th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/software-engineering-10th-ed./</guid><description>&lt;p&gt;A broad overview of software engineering principles, processes, and management.&lt;/p&gt;</description></item><item><title>Structure and Interpretation of Computer Programs</title><link>https://lbenicio.dev/reading/structure-and-interpretation-of-computer-programs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/structure-and-interpretation-of-computer-programs/</guid><description>&lt;p&gt;Classic text teaching fundamental concepts of programming and abstraction using Scheme.&lt;/p&gt;</description></item><item><title>Structure and Interpretation of Computer Programs</title><link>https://lbenicio.dev/reading/structure-and-interpretation-of-computer-programs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/structure-and-interpretation-of-computer-programs/</guid><description>&lt;p&gt;A classic text that teaches fundamental programming concepts and abstraction using Scheme.&lt;/p&gt;</description></item><item><title>Structured Computer Organization (6th ed.)</title><link>https://lbenicio.dev/reading/structured-computer-organization-6th-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/structured-computer-organization-6th-ed./</guid><description>&lt;p&gt;A foundational text on computer organization bridging hardware and software.&lt;/p&gt;</description></item><item><title>The Art of Computer Programming, Vols. 1–4A</title><link>https://lbenicio.dev/reading/the-art-of-computer-programming-vols.-14a/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/the-art-of-computer-programming-vols.-14a/</guid><description>&lt;p&gt;Foundational multi-volume work on algorithms, data structures, and analysis of algorithms.&lt;/p&gt;</description></item><item><title>The Pragmatic Programmer (20th Anniversary ed.)</title><link>https://lbenicio.dev/reading/the-pragmatic-programmer-20th-anniversary-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/the-pragmatic-programmer-20th-anniversary-ed./</guid><description>&lt;p&gt;Timeless advice on becoming an effective, pragmatic software developer.&lt;/p&gt;</description></item><item><title>Using MPI: Portable Programming with the Message Passing Interface (3rd ed.)</title><link>https://lbenicio.dev/reading/using-mpi-portable-programming-with-the-message-passing-interface-3rd-ed./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/using-mpi-portable-programming-with-the-message-passing-interface-3rd-ed./</guid><description>&lt;p&gt;Hands-on guide to MPI for building distributed-memory parallel programs, complementing OpenMP.&lt;/p&gt;</description></item><item><title>Using OpenMP: Portable Shared Memory Parallel Programming</title><link>https://lbenicio.dev/reading/using-openmp-portable-shared-memory-parallel-programming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://lbenicio.dev/reading/using-openmp-portable-shared-memory-parallel-programming/</guid><description>&lt;p&gt;Practical OpenMP techniques, performance tips, and patterns for shared-memory parallel programming.&lt;/p&gt;</description></item></channel></rss>